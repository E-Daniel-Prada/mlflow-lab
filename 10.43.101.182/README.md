# ğŸš€ MLflow API - Inferencia y GestiÃ³n de Modelos

Este proyecto implementa una API basada en **FastAPI** para la gestiÃ³n y consumo de modelos de Machine Learning utilizando **MLflow**. Permite realizar inferencias con modelos registrados, consultar experimentos y gestionar modelos en un entorno de despliegue.

---

## ğŸ“‚ Estructura del Proyecto

```
mlflow-api/
â”‚â”€â”€ Dockerfile                   # ConfiguraciÃ³n de la imagen Docker
â”‚â”€â”€ docker-compose.yml           # OrquestaciÃ³n de servicios en Docker
â”‚â”€â”€ requirements.txt             # Dependencias necesarias para la API
â”‚â”€â”€ .env                         # Variables de entorno
â”‚â”€â”€ app/                         # CÃ³digo de la API
â”‚   â”œâ”€â”€ main.py                  # CÃ³digo principal de la API (FastAPI)
â”‚   â”œâ”€â”€ inference.py             # LÃ³gica de inferencia y carga de modelos
â”‚   â”œâ”€â”€ __init__.py              # Archivo de inicializaciÃ³n del mÃ³dulo
```

---

## ğŸ“Œ **Funcionalidades de la API**

âœ… **Carga automÃ¡tica de modelos registrados en MLflow**  
âœ… **RealizaciÃ³n de predicciones con los modelos disponibles**  
âœ… **Consulta de experimentos registrados en MLflow**  
âœ… **GestiÃ³n de modelos para selecciÃ³n del mÃ¡s reciente**  
âœ… **Despliegue con Docker y Docker Compose**  

---

## ğŸ”§ **InstalaciÃ³n y ConfiguraciÃ³n**

### 1ï¸âƒ£ **Pre-requisitos**
- Docker
- Docker Compose
- Python 3.10 o superior (si se ejecuta fuera de Docker)

### 2ï¸âƒ£ **ConfiguraciÃ³n del entorno**
El archivo `.env` debe contener la URL del servidor MLflow:
```env
MLFLOW_TRACKING_URI=http://10.43.101.184:5000
```

---

## ğŸš€ **Despliegue de la API con Docker**

### ğŸ”¹ **1ï¸âƒ£ Construir y ejecutar los contenedores**
```bash
sudo docker-compose up -d --build
```

### ğŸ”¹ **2ï¸âƒ£ Verificar los logs del servicio**
```bash
docker logs mlflow_api
```

### ğŸ”¹ **3ï¸âƒ£ Verificar que la API estÃ© corriendo**
```bash
curl -X 'GET' 'http://localhost:8888'
```
Salida esperada:
```json
{"message": "API de Inferencia y Experimentos con MLflow estÃ¡ corriendo"}
```

---

## ğŸ” **Endpoints Disponibles**

### ğŸ”¹ **1ï¸âƒ£ Listar Modelos Registrados**
```bash
curl -X 'GET' 'http://localhost:8888/models'
```
ğŸ“Œ **Respuesta esperada:**
```json
{"models": ["random_forest_model"]}
```

### ğŸ”¹ **2ï¸âƒ£ Realizar una PredicciÃ³n**
```bash
curl -X 'POST' 'http://localhost:8888/predict/' \
  -H 'Content-Type: application/json' \
  -d '{"features": [3305, 35, 13, 134, 25, 5811, 218, 211, 127, 659, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]}'
```
ğŸ“Œ **Respuesta esperada:**
```json
{"prediction": [2]}
```

### ğŸ”¹ **3ï¸âƒ£ Obtener InformaciÃ³n de un Experimento**
```bash
curl -X 'GET' 'http://localhost:8888/experiment/random_forest_model'
```

### ğŸ”¹ **4ï¸âƒ£ Obtener InformaciÃ³n de una EjecuciÃ³n en MLflow**
```bash
curl -X 'GET' 'http://localhost:8888/run/{run_id}'
```

---

## ğŸ” **Funcionamiento Interno**

### ğŸ“Œ **Carga AutomÃ¡tica del Ãšltimo Modelo en MLflow**
El sistema detecta automÃ¡ticamente el modelo mÃ¡s reciente en MLflow y lo carga en memoria:
```python
mlflow.set_tracking_uri(os.getenv("MLFLOW_TRACKING_URI"))
client = MlflowClient()
model_uri = f"models:/{model_name}/latest"
model = mlflow.pyfunc.load_model(model_uri)
```

### ğŸ“Œ **Inferencia con el Modelo**
El endpoint `/predict/` recibe datos en formato JSON y realiza la predicciÃ³n con el modelo cargado:
```python
def make_prediction(model, input_data):
    if model is None:
        return {"error": "No hay modelos disponibles para hacer inferencias."}
    prediction = model.predict(input_data)
    return {"prediction": prediction.tolist()}
```

---

## ğŸ¯ **ConclusiÃ³n**
Este proyecto permite la integraciÃ³n con **MLflow** para la gestiÃ³n y consumo de modelos de Machine Learning, facilitando el despliegue y uso de modelos en producciÃ³n mediante una API accesible. ğŸš€


